# Colab-friendly: Intelligent Excuse Generator (Gradio)
# Install deps in Colab if needed:
# !pip install gradio gTTS pillow reportlab faker langdetect googletrans==4.0.0-rc1 transformers sentencepiece

import io
import re
import random
import datetime
import logging

import gradio as gr
from faker import Faker
from gtts import gTTS
from PIL import Image, ImageDraw, ImageFont
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas as pdf_canvas

# optional: translation + lang detect
try:
    from googletrans import Translator
except Exception:
    Translator = None

try:
    from langdetect import detect
except Exception:
    detect = None

# optional: transformers for text2text & sentiment
try:
    from transformers import pipeline
    HF_AVAILABLE = True
except Exception:
    HF_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ieg_gradio")

fake = Faker()
translator = Translator() if Translator else None
RANDOM_SEED = 42
random.seed(RANDOM_SEED)

# ---------------- Utilities ----------------
def clean_text(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip())

def extract_keywords(text: str, top_n: int = 6) -> list:
    txt = re.sub(r"[^a-zA-Z0-9 ]", " ", (text or "").lower())
    tokens = [t for t in txt.split() if len(t) > 2]
    stopwords = set([
        'the','and','for','you','was','had','that','this','with','but','not','from','are','they','their','your','because'
    ])
    tokens = [t for t in tokens if t not in stopwords]
    freq = {}
    for t in tokens:
        freq[t] = freq.get(t, 0) + 1
    sorted_tokens = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [t for t,_ in sorted_tokens[:top_n]]

def analyze_sentiment(text: str) -> dict:
    txt = clean_text(text)
    # Simple fallback because adding HF sentiment models in Colab is heavy
    positive = ['good','better','ok','fine','improved','fortunate','happy']
    negative = ['bad','ill','sick','urgent','emergency','broken','angry','sad','missed','late']
    score = 0
    for w in positive:
        if w in txt: score += 1
    for w in negative:
        if w in txt: score -= 1
    label = 'POSITIVE' if score > 0 else 'NEGATIVE' if score < 0 else 'NEUTRAL'
    return {"label": label, "score": abs(float(score))}

def choose_mood_from_context(context: str) -> str:
    s = analyze_sentiment(context)
    low_ctx = (context or "").lower()
    if any(x in low_ctx for x in ['emergency','hospital','urgent','accident']):
        return 'Apologetic'
    if s['label'] == 'NEGATIVE':
        return 'Sad'
    if any(token in ['angry','frustrated','mad'] for token in extract_keywords(context, 8)):
        return 'Angry'
    return 'Neutral'

# Templates fallback
TEMPLATES = [
    "I apologize for missing the {scenario}. I was dealing with {context}. I understand this caused inconvenience and I take responsibility.",
    "Please accept my apologies — I could not attend {scenario} because of {context}. It was unexpected and I did my best to handle it.",
    "I'm sorry I missed {scenario}. Due to {context}, I couldn't make it. I'll make sure to catch up on anything I missed.",
    "Regrettably, I was absent from {scenario} because {context}. I hope you can understand and I'll ensure it won't happen again."
]

def score_template(template: str, context: str, desired_mood: str) -> float:
    k = extract_keywords(context, top_n=6)
    score = 0.0
    for token in k:
        if token in template.lower():
            score += 1.0
    if desired_mood == 'Apologetic' and 'apolog' in template.lower():
        score += 1.0
    score += random.random() * 0.01
    return score

# ---------------- Model wrappers ----------------
text2text_gen = None
if HF_AVAILABLE:
    try:
        text2text_gen = pipeline("text2text-generation", model="google/flan-t5-base")
    except Exception as e:
        logger.warning("Could not load flan-t5: %s", e)
        text2text_gen = None

def generate_with_model(prompt: str, max_length: int = 100):
    if text2text_gen:
        try:
            out = text2text_gen(prompt, max_length=max_length, do_sample=True)[0]
            return out.get('generated_text', '').strip()
        except Exception as e:
            logger.warning("Model generation failed: %s", e)
    # fallback: template-based
    template = random.choice(TEMPLATES)
    # try to infer scenario/context fragments from prompt safely
    try:
        scenario_part = prompt.split("for")[1].split(":")[0].strip()
    except Exception:
        scenario_part = "the session"
    try:
        context_part = prompt.split(":")[-1].strip()
    except Exception:
        context_part = "unforeseen circumstances"
    return template.format(scenario=scenario_part, context=context_part)

def generate_apology(context: str):
    p = f"Write a short apologetic sentence for: {context}"
    return generate_with_model(p, max_length=60)

def generate_emergency_excuse():
    emergencies = [
        "Family medical emergency",
        "Fire evacuation at home",
        "Sudden illness requiring rest",
        "Power outage in the neighborhood",
        "Accident on the commute",
        "Car breakdown"
    ]
    return fake.random_element(elements=emergencies)

# ---------------- Artifacts (with textbbox) ----------------
def make_audio_bytes(text: str) -> io.BytesIO:
    buf = io.BytesIO()
    try:
        tts = gTTS(text)
        tts.write_to_fp(buf)
        buf.seek(0)
        return buf
    except Exception as e:
        logger.warning("gTTS failed: %s", e)
        return io.BytesIO()

def make_certificate_png_bytes(excuse_text: str, title: str = "Certified Excuse") -> bytes:
    width, height = 1200, 600
    img = Image.new('RGB', (width, height), color='white')
    draw = ImageDraw.Draw(img)
    try:
        font_title = ImageFont.truetype("arial.ttf", 48)
        font_body = ImageFont.truetype("arial.ttf", 22)
    except Exception:
        font_title = ImageFont.load_default()
        font_body = ImageFont.load_default()

    # title
    draw.text((60, 40), title, font=font_title, fill='black')

    # ---- wrap using textbbox (safe for Pillow 10+) ----
    max_width = width - 120
    words = (excuse_text or "").split()
    lines = []
    cur = ""
    for w in words:
        test_line = (cur + " " + w).strip()
        bbox = draw.textbbox((0, 0), test_line, font=font_body)
        text_width = bbox[2] - bbox[0]
        if text_width <= max_width:
            cur = test_line
        else:
            if cur:
                lines.append(cur)
            cur = w
    if cur:
        lines.append(cur)

    # draw wrapped lines
    y = 140
    line_height = (draw.textbbox((0,0), "Ay", font=font_body)[3] - draw.textbbox((0,0), "Ay", font=font_body)[1]) + 8
    for line in lines:
        draw.text((60, y), line, font=font_body, fill='darkred')
        y += line_height
        if y > height - 120:
            break

    # footer
    footer_text = f"Generated on {datetime.date.today().isoformat()}"
    draw.text((width - 420, height - 80), footer_text, font=font_body, fill='black')

    bio = io.BytesIO()
    img.save(bio, format='PNG')
    bio.seek(0)
    return bio.read()

def make_certificate_pdf_bytes(excuse_text: str) -> bytes:
    buffer = io.BytesIO()
    c = pdf_canvas.Canvas(buffer, pagesize=letter)
    width, height = letter
    c.setFont('Helvetica-Bold', 20)
    c.drawString(72, height - 72, 'Certified Excuse')
    c.setFont('Helvetica', 12)
    textobject = c.beginText(72, height - 120)
    for line in re.findall('.{1,80}(?:\\s|$)', excuse_text or ""):
        textobject.textLine(line.strip())
    c.drawText(textobject)
    c.setFont('Helvetica-Oblique', 10)
    c.drawString(72, 72, f"Generated on {datetime.date.today().isoformat()}")
    c.showPage()
    c.save()
    buffer.seek(0)
    return buffer.read()

# ---------------- History store ----------------
EXCUSE_HISTORY = []

# ---------------- Main pipeline ----------------
def excuse_pipeline(context, scenario, urgency, mood, tone, lang_code, variants=3, use_model=True):
    context = clean_text(context)
    scenario = clean_text(scenario) or "the session"
    urgency = urgency or "Medium"
    mood = mood or "Auto"
    tone = tone or "Formal"
    lang_code = (lang_code or "en").strip()

    if mood == "Auto":
        mood = choose_mood_from_context(context)

    base_prompt = f"{urgency} urgency, {mood} mood, {tone} tone excuse for {scenario}: {context}"
    variants_list = []

    # Start with template-based variants
    for t in TEMPLATES:
        txt = t.format(scenario=scenario, context=context or "unforeseen circumstances")
        variants_list.append({"text": txt, "score": score_template(t, context, mood)})

    # Add model variants if allowed
    if use_model:
        for _ in range(max(0, int(variants) - len(variants_list))):
            gm = generate_with_model(base_prompt, max_length=120)
            if urgency.lower() == 'high' and 'urgent' not in gm.lower():
                gm = gm + f" (This was a {urgency.lower()} matter.)"
            variants_list.append({"text": gm, "score": random.random()})

    variants_sorted = sorted(variants_list, key=lambda x: x['score'], reverse=True)[:int(variants)]
    chosen = variants_sorted[0]['text'] if variants_sorted else TEMPLATES[0].format(scenario=scenario, context=context)

    apology = generate_apology(context)
    emergency = generate_emergency_excuse()

    # translation best-effort
    translated = chosen
    if translator and lang_code and lang_code != 'en':
        try:
            translated = translator.translate(chosen, dest=lang_code).text
        except Exception as e:
            logger.warning("Translation failed: %s", e)
            translated = chosen

    # language detection best-effort
    lang_detected = "unknown"
    if detect:
        try:
            lang_detected = detect(chosen)
        except Exception:
            lang_detected = "unknown"

    audio_buf = make_audio_bytes(chosen)
    png_bytes = make_certificate_png_bytes(chosen)
    pdf_bytes = make_certificate_pdf_bytes(chosen)

    analysis = {
        "keywords": extract_keywords(context),
        "sentiment": analyze_sentiment(context),
        "detected_mood": mood,
        "variants_count": len(variants_sorted)
    }

    EXCUSE_HISTORY.append({
        "timestamp": datetime.datetime.utcnow().isoformat(),
        "context": context,
        "scenario": scenario,
        "chosen": chosen,
        "analysis": analysis
    })

    return {
        "excuse": chosen,
        "translated": translated,
        "apology": apology,
        "emergency": emergency,
        "detected_lang": lang_detected,
        "audio": audio_buf,
        "certificate_png": png_bytes,
        "certificate_pdf": pdf_bytes,
        "analysis": analysis
    }

# ---------------- Gradio UI ----------------
with gr.Blocks(title="Intelligent Excuse Generator (IEG)") as demo:
    gr.Markdown("## Intelligent Excuse Generator (IEG)\nContext-aware excuses with downloadable audio and certificates — Colab friendly.")
    with gr.Row():
        with gr.Column(scale=2):
            ctx = gr.Textbox(label="Context (reason)", placeholder="e.g., Missed deadline because my internet went down", lines=4)
            scen = gr.Textbox(label="Scenario", placeholder="e.g., Work, Class, Interview")
            with gr.Row():
                urg = gr.Dropdown(choices=["Low", "Medium", "High"], label="Urgency", value="Medium")
                mood = gr.Dropdown(choices=["Auto", "Neutral", "Sad", "Angry", "Apologetic"], label="Mood", value="Auto")
                tone = gr.Dropdown(choices=["Formal", "Casual"], label="Tone", value="Formal")
            lang = gr.Textbox(label="Target language code for translation (e.g., 'fr', 'hi')", value="en")
            variants = gr.Slider(minimum=1, maximum=5, step=1, label="Number of variants to generate", value=3)
            use_model = gr.Checkbox(label="Use text2text model (flan-t5) if available", value=True)
            generate_btn = gr.Button("Generate Excuse", variant="primary")

        with gr.Column(scale=3):
            out_excuse = gr.Textbox(label="Generated Excuse", lines=3)
            out_translated = gr.Textbox(label="Translated Excuse", lines=3)
            out_apology = gr.Textbox(label="Apology (short)")
            out_emergency = gr.Textbox(label="Emergency excuse (short)")
            out_lang = gr.Textbox(label="Detected Language")
            out_audio = gr.Audio(label="Audio (MP3)", interactive=False)
            with gr.Row():
                out_png = gr.Image(label="Certificate (PNG)")
                out_pdf = gr.File(label="Certificate (PDF)")
            out_analysis = gr.JSON(label="Analysis (keywords, sentiment, chosen mood)")

    def on_generate(context, scenario, urgency, mood, tone, lang_code, variants, use_model_flag):
        res = excuse_pipeline(context, scenario, urgency, mood, tone, lang_code, variants=int(variants), use_model=use_model_flag)
        # Return values Gradio expects:
        audio_val = (res["audio"], "audio/mp3") if res["audio"] and isinstance(res["audio"], io.BytesIO) else None
        pdf_file = io.BytesIO(res["certificate_pdf"]) if isinstance(res["certificate_pdf"], (bytes, bytearray)) else res["certificate_pdf"]
        return (
            res["excuse"], res["translated"], res["apology"], res["emergency"], res["detected_lang"],
            audio_val, res["certificate_png"], pdf_file, res["analysis"]
        )

    generate_btn.click(
        fn=on_generate,
        inputs=[ctx, scen, urg, mood, tone, lang, variants, use_model],
        outputs=[out_excuse, out_translated, out_apology, out_emergency, out_lang, out_audio, out_png, out_pdf, out_analysis]
    )

    gr.Markdown("**History (in-memory)** — refresh to view last 10.")
    hist_table = gr.Dataframe(value=[], label="Generation history (timestamp, scenario, chosen)")
    refresh = gr.Button("Refresh history")
    def get_history_rows():
        rows = []
        for h in EXCUSE_HISTORY[-10:][::-1]:
            rows.append({"timestamp": h["timestamp"], "scenario": h["scenario"], "chosen": h["chosen"]})
        return rows
    refresh.click(lambda: get_history_rows(), None, hist_table)

# Launch
if __name__ == "__main__":
    demo.launch(share=False)
